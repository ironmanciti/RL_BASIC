{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c46d28cd",
   "metadata": {},
   "source": [
    "# 022_MC_blackjack_control\n",
    "## Suntton p.101 \n",
    "### On-Policy First-Visit MC control(for e-soft policies) for optimum policy pi*\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/9985DE425C7C66AD28\" width=600 />\n",
    "\n",
    "- e-soft policy $\\pi$, state-action value Q(s, a) 를 random 하게 초기화   \n",
    "\n",
    "- 각 state-action pair  로 부터의 return 을 저장할 Returns(s, a) empty list 생성  \n",
    "\n",
    "- Policy $\\pi$ 를 이용하여 episode 생성\n",
    "- 각 state-action pair 에 대해, first occurrence 이후의 return 들을 더함. Q(s, a) = first occurrence 이후의 모든 return 의 평균.\n",
    "- 각 state 에 대하여 Policy 가 해당 state 의 가장 valuable 한 action을 선택할 확률 증가시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215d7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "#state : (player card sum, dealer open card, usable_ace 보유) ex) (6, 1, False)\n",
    "win_cnt = 0\n",
    "lose_cnt = 0\n",
    "draw_cnt = 0\n",
    "num_episodes = 100_000\n",
    "GAMMA = 1  # no discount\n",
    "\n",
    "#Algorithm parameter: small e > 0\n",
    "e = 0.2 \n",
    "\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd447fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 completed...\n",
      "episode 5000 completed...\n",
      "episode 10000 completed...\n",
      "episode 15000 completed...\n",
      "episode 20000 completed...\n",
      "episode 25000 completed...\n",
      "episode 30000 completed...\n",
      "episode 35000 completed...\n",
      "episode 40000 completed...\n",
      "episode 45000 completed...\n",
      "episode 50000 completed...\n",
      "episode 55000 completed...\n",
      "episode 60000 completed...\n",
      "episode 65000 completed...\n",
      "episode 70000 completed...\n",
      "episode 75000 completed...\n",
      "episode 80000 completed...\n",
      "episode 85000 completed...\n",
      "episode 90000 completed...\n",
      "episode 95000 completed...\n"
     ]
    }
   ],
   "source": [
    "#Initialize\n",
    "#pi <- an arbitrary e-soft policy (초기 policy)\n",
    "#Q(s,a) 초기화 for all s, a\n",
    "#Returns(s, a) <- empty list for all s, a\n",
    "pi = defaultdict(lambda: np.ones(num_actions) / num_actions)\n",
    "Q = defaultdict(lambda: np.zeros(num_actions))\n",
    "Returns = defaultdict(list)\n",
    "\n",
    "#Repeat forever (for each episode)\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    #Generate an episode following pi: S0,A0,R1,S1,A1,R2,..ST-1,AT-1,RT\n",
    "    episode = []\n",
    "    s = env.reset() #s:(sum_hand(player), dealer open card, usable_ace 보유)\n",
    "    while True:\n",
    "        p = pi[s]\n",
    "        a = np.random.choice(np.arange(len(p)), p=p)  # 0:stick, 1:hit\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        episode.append((s, a, r))\n",
    "        if done: \n",
    "            # 80% episode 동안 policy 개선된 후 win/lose count\n",
    "            if i > 0.8 * num_episodes: \n",
    "                if r == 1:\n",
    "                    win_cnt += 1\n",
    "                elif r == -1:\n",
    "                    lose_cnt += 1\n",
    "                else:\n",
    "                    draw_cnt += 1\n",
    "            break\n",
    "        s = s_\n",
    "        \n",
    "    #G <- 0\n",
    "    G = 0\n",
    "    #Loop for each step of episode, t=T-1, T-2,...0\n",
    "    for s, a, r in episode[::-1]:\n",
    "        # G <- gamma*G + R_(t+1)\n",
    "        G = GAMMA * G + r\n",
    "        visited_state_action_pair = []\n",
    "        #Unless the pair S_t, A_t appears in S_0,A_0 S_1,A_1..S_(t-1),A_(t-1):\n",
    "            #Append G to Returns(S_t, A_t)\n",
    "            #Q(S_t,A_t) <- average(Returns(S_t, A_t))\n",
    "        if (s, a) not in visited_state_action_pair:\n",
    "            Returns[(s, a)].append(G)\n",
    "            Q[s][a] = np.mean(Returns[(s, a)])\n",
    "            visited_state_action_pair.append((s, a))\n",
    "        \n",
    "        #A* <- argmax_a Q(S_t,a)\n",
    "        A_star = np.argmax(Q[s])\n",
    "        #For all a:\n",
    "            #pi(a|S_t) <- 1-e + e/|A(S_t)| if a = A*\n",
    "            #          <- e/|A(St)|        if a != A*\n",
    "        for a in range(num_actions):\n",
    "            if a == A_star:\n",
    "                pi[s][a] = 1 - e + e/num_actions\n",
    "            else:\n",
    "                pi[s][a] = e/num_actions\n",
    "                \n",
    "    if i % 5000 == 0:\n",
    "        print(f\"episode {i} completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f71845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win ratio = 40.27%\n",
      "lose ratio = 51.90%\n",
      "draw ratio = 7.83%\n"
     ]
    }
   ],
   "source": [
    "print(\"win ratio = {:.2f}%\".format(win_cnt/(0.2 * num_episodes)*100))\n",
    "print(\"lose ratio = {:.2f}%\".format(lose_cnt/(0.2 * num_episodes)*100))\n",
    "print(\"draw ratio = {:.2f}%\".format(draw_cnt/(0.2 * num_episodes)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cf75a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state (21, 3, True)의 가치 = 0.98 stick\n",
      "state (4, 1, False)의 가치 = -0.67 hit\n",
      "state (14, 8, True)의 가치 = -0.18 hit\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "#state : (player card sum, dealer open card, usable_ace 보유) \n",
    "\n",
    "sample_state = (21, 3, True)\n",
    "optimal_action = np.argmax(Q[sample_state])\n",
    "print(\"state {}의 가치 = {:.2f}\".format(sample_state, Q[sample_state][optimal_action]),\n",
    "      \"stick\" if optimal_action == 0 else \"hit\")\n",
    "\n",
    "sample_state = (4, 1, False)\n",
    "optimal_action = np.argmax(Q[sample_state])\n",
    "print(\"state {}의 가치 = {:.2f}\".format(sample_state, Q[sample_state][optimal_action]),\n",
    "      \"stick\" if optimal_action == 0 else \"hit\")\n",
    "\n",
    "sample_state = (14, 8, True)\n",
    "optimal_action = np.argmax(Q[sample_state])\n",
    "print(\"state {}의 가치 = {:.2f}\".format(sample_state, Q[sample_state][optimal_action]),\n",
    "      \"stick\" if optimal_action == 0 else \"hit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac2a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
