{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c32b1e",
   "metadata": {},
   "source": [
    "# 015_DP_frozenlake_policy_evaluation\n",
    "# Iterative Policy Evaluation\n",
    "## One-Array version\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "nA = 4\n",
    "nS = 4*4 = 16\n",
    "P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "env.P[0][0] \n",
    "{0: {0: [(0.3333333333333333, 0, 0.0, False), --> (prob, s_, reward, done)\n",
    "         (0.3333333333333333, 0, 0.0, False),\n",
    "         (0.3333333333333333, 4, 0.0, False)],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c08c4513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 4, 0.0, False)],\n",
       "  2: [(1.0, 1, 0.0, False)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 1: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 5, 0.0, True)],\n",
       "  2: [(1.0, 2, 0.0, False)],\n",
       "  3: [(1.0, 1, 0.0, False)]},\n",
       " 2: {0: [(1.0, 1, 0.0, False)],\n",
       "  1: [(1.0, 6, 0.0, False)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 3: {0: [(1.0, 2, 0.0, False)],\n",
       "  1: [(1.0, 7, 0.0, True)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 3, 0.0, False)]},\n",
       " 4: {0: [(1.0, 4, 0.0, False)],\n",
       "  1: [(1.0, 8, 0.0, False)],\n",
       "  2: [(1.0, 5, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(1.0, 5, 0.0, True)],\n",
       "  1: [(1.0, 10, 0.0, False)],\n",
       "  2: [(1.0, 7, 0.0, True)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 12, 0.0, True)],\n",
       "  2: [(1.0, 9, 0.0, False)],\n",
       "  3: [(1.0, 4, 0.0, False)]},\n",
       " 9: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 10, 0.0, False)],\n",
       "  3: [(1.0, 5, 0.0, True)]},\n",
       " 10: {0: [(1.0, 9, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 11, 0.0, True)],\n",
       "  3: [(1.0, 6, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(1.0, 12, 0.0, True)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 14, 0.0, False)],\n",
       "  3: [(1.0, 9, 0.0, False)]},\n",
       " 14: {0: [(1.0, 13, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 15, 1.0, True)],\n",
       "  3: [(1.0, 10, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66465063",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = len(env.P)\n",
    "num_actions = len(env.P[0])\n",
    "transitions = env.P "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d63ec65f",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*G1mg6UU6wsLlJApebP3BMg.png\" width=600/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5c59c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 1.0\n",
    "\n",
    "#Input pi, the policy to be evaluated \n",
    "policy = np.ones([num_states, num_actions]) * 0.25\n",
    "\n",
    "#Algorithm parameter: a small threshold theta > 0\n",
    "THETA = 1e-5\n",
    "\n",
    "# initialize an array V(s) = 0 for all s in S+, arbitrarily except that V(terminal) = 0\n",
    "V = np.zeros(num_states)\n",
    "\n",
    "#Loop\n",
    "while True:\n",
    "    #delta <- 0\n",
    "    delta = 0\n",
    "    #Loop for each s:\n",
    "    for s in range(num_states):\n",
    "        #v <- V(s)\n",
    "        old_value = V[s]\n",
    "        new_value = 0\n",
    "        #update rule : V(s) = sum(pi(a|s)*sum(p(s,a)*[r + gamma*v(s')]))\n",
    "        for a, prob_action in enumerate(policy[s]):\n",
    "            # sum over s', r\n",
    "            for prob, s_, reward, _ in transitions[s][a]:\n",
    "                new_value += prob_action * prob * (reward + GAMMA * V[s_])\n",
    "        V[s] = new_value\n",
    "        #delta <- max(delta|v - V(s)|)\n",
    "        delta = max(delta, np.abs(old_value - V[s]))\n",
    "        \n",
    "    #until delta < theta\n",
    "    if delta < THETA:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5b15fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수렴한 Optimal Value = \n",
      " [[0.013911   0.01161424 0.02094062 0.01046758]\n",
      " [0.01623478 0.         0.04074774 0.        ]\n",
      " [0.03479961 0.08816698 0.14205099 0.        ]\n",
      " [0.         0.17581855 0.4392897  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#V는 v_pi에 수렴\n",
    "V = V.reshape(4, 4)\n",
    "print(\"수렴한 Optimal Value = \\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0c14b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0139, 0.0116, 0.0209, 0.0105, ]\n",
      "[0.0162, 0.0000, 0.0407, 0.0000, ]\n",
      "[0.0348, 0.0882, 0.1421, 0.0000, ]\n",
      "[0.0000, 0.1758, 0.4393, 0.0000, ]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print('[', end='')\n",
    "    for j in range(4):\n",
    "        print(f\"{V[i, j]:.4f}\", end=', ')\n",
    "    print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ecbdee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
