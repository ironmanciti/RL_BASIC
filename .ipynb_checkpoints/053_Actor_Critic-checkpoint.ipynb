{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select step-size parameters 0<alpha<=1\n",
    "- Choose discount rate 0<gamma<=1\n",
    "- Choose max number of episodes N\n",
    "- Choose number of episodes to batch together for an update K >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v0')\n",
    "# alpha = 0.001\n",
    "# gamma = 0.99\n",
    "# num_episodes = 2000\n",
    "# batch_size = 10\n",
    "env = gym.make('LunarLander-v2')\n",
    "alpha = 0.0001\n",
    "gamma = 0.99\n",
    "num_episodes = 300\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic_network(nn.Module):\n",
    "    def __init__(self, env, fc1_dims=256):\n",
    "        super(ActorCritic_network, self).__init__()\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.fc1 = nn.Linear(self.n_inputs, fc1_dims)\n",
    "        self.fc_pi = nn.Linear(fc1_dims, self.n_outputs)\n",
    "        self.fc_v = nn.Linear(fc1_dims, 1)\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "    def pi(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=-1)\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def v(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        v = self.fc_v(x)   \n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCritic_network(\n",
       "  (fc1): Linear(in_features=8, out_features=256, bias=True)\n",
       "  (fc_pi): Linear(in_features=256, out_features=4, bias=True)\n",
       "  (fc_v): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ActorCritic_network(env).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=alpha)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta^{\\pi\\theta}= r + \\gamma V^{\\pi\\theta}(S^{\\prime}) - V^{\\pi\\theta}(S)$ \n",
    "\n",
    "Actor의 Loss --> $\\log{\\pi\\theta(s,a)}\\delta^{\\pi\\theta}$ 를 maximize\n",
    "\n",
    "Critic의 Loss -->$\\delta^{\\pi\\theta}$ 를 minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
    "    for s, a, r, s_prime, done in batch_data:\n",
    "        s_lst.append(s)\n",
    "        a_lst.append([a])\n",
    "        r_lst.append([r/100.0])\n",
    "        s_prime_lst.append(s_prime)\n",
    "        done_lst.append([done])\n",
    "        \n",
    "    s_tensor = T.FloatTensor(s_lst).to(device)\n",
    "    a_tensor = T.LongTensor(a_lst).to(device)\n",
    "    r_tensor = T.FloatTensor(r_lst).to(device)\n",
    "    s_prime_tensor = T.FloatTensor(s_prime_lst).to(device)\n",
    "    done_tensor = T.LongTensor(done_lst).to(device)\n",
    "    \n",
    "    return s_tensor, a_tensor, r_tensor, s_prime_tensor, done_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#to visualize last 100 episode\n",
    "total_rewards = []\n",
    "# for batch update\n",
    "batch_data = []\n",
    "batch_counter = 0\n",
    "screen = None\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    cur_state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    \n",
    "    while not done:\n",
    "        prob = net.pi(T.FloatTensor(cur_state).to(device))\n",
    "        m = Categorical(prob)\n",
    "        action = m.sample().item()\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "#         if episode > num_episodes * 0.98:\n",
    "#             if screen is None:\n",
    "#                 plt.figure(figsize=(6,4))\n",
    "#                 screen = env.render(mode='rgb_array')\n",
    "#                 img = plt.imshow(screen) # only call this once\n",
    "                \n",
    "#             img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "#             display.display(plt.gcf())\n",
    "#             display.clear_output(wait=True)\n",
    "\n",
    "        batch_data.append((cur_state, action, reward, next_state, done))\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        cur_state = next_state\n",
    "        \n",
    "        if done: # one episode terminated\n",
    "            batch_counter += 1\n",
    "            total_rewards.append(sum(rewards))\n",
    "            \n",
    "            if batch_counter % batch_size == 0:\n",
    "                s, a, r, s_prime, d = make_batch()\n",
    "                \n",
    "                td_target = r + gamma * net.v(s_prime) * d   # terminal state의 value=0\n",
    "                delta = td_target - net.v(s)\n",
    "                \n",
    "                pi = net.pi(s)\n",
    "                pi_a = pi.gather(1, a)\n",
    "                loss = -T.log(pi_a) * delta.detach() #+ delta * delta #F.smooth_l1_loss(net.v(s), td_target.detach())\n",
    "                              \n",
    "                optimizer.zero_grad()\n",
    "                loss.mean().backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_data = []\n",
    "                \n",
    "                avg_rewards = np.mean(total_rewards[-100:])\n",
    "                print(\"Episode {} - average of last 100: {:.2f}\".format(episode, avg_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = [np.mean(total_rewards[i+100:i+200]) for i in range(len(total_rewards)-100)]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(total_rewards, label='rewards for episode')\n",
    "plt.plot(mean_rewards, label='average reward of last 100 episodes')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Rewards\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
