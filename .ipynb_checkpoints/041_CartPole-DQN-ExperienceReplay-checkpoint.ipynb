{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfae632",
   "metadata": {},
   "source": [
    "# Deep Q-learning with Experience Replay - Deep Mind\n",
    "### Playing Atari with Deep Reinforcement Learning - 2013.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a79e6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "\n",
    "env = gym.make('CartPole-v1')  \n",
    "\n",
    "seed_val = 23\n",
    "env.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "random.seed(seed_val)\n",
    "#--- hyper-parameters -----\n",
    "num_episodes = 100  # v0-100, v1-150 episodes\n",
    "gamma = 0.999\n",
    "learning_rate = 0.001  # faster or stable training\n",
    "replay_memory_size = 50000\n",
    "batch_size = 128\n",
    "hidden_layer = 64\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "\n",
    "#---------------------\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device = {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc9d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = env.observation_space.shape[0]  # 4\n",
    "n_outputs = env.action_space.n  # 2\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "\n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, hidden_layer//2)\n",
    "        self.linear3 = nn.Linear(hidden_layer//2, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = F.relu(self.linear1(x))\n",
    "        a2 = F.relu(self.linear2(a1))\n",
    "        output = self.linear3(a2)\n",
    "        return output\n",
    "\n",
    "def select_action(state, steps_done):\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            state = torch.Tensor(state).to(device)\n",
    "            action_values = Q(state)\n",
    "            action = torch.argmax(action_values).item()\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8548ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episode finished after 19.0 rewards\n",
      "1 episode finished after 41.0 rewards\n",
      "2 episode finished after 11.0 rewards\n",
      "3 episode finished after 15.0 rewards\n",
      "4 episode finished after 14.0 rewards\n",
      "5 episode finished after 10.0 rewards\n",
      "6 episode finished after 10.0 rewards\n",
      "7 episode finished after 9.0 rewards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trimu\\AppData\\Local\\Temp/ipykernel_27312/990643817.py:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  states = torch.Tensor(states).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 episode finished after 10.0 rewards\n",
      "9 episode finished after 12.0 rewards\n",
      "10 episode finished after 10.0 rewards\n",
      "11 episode finished after 11.0 rewards\n",
      "12 episode finished after 10.0 rewards\n",
      "13 episode finished after 10.0 rewards\n",
      "14 episode finished after 10.0 rewards\n",
      "15 episode finished after 11.0 rewards\n",
      "16 episode finished after 13.0 rewards\n",
      "17 episode finished after 13.0 rewards\n",
      "18 episode finished after 10.0 rewards\n",
      "19 episode finished after 10.0 rewards\n",
      "20 episode finished after 9.0 rewards\n",
      "21 episode finished after 14.0 rewards\n",
      "22 episode finished after 11.0 rewards\n",
      "23 episode finished after 13.0 rewards\n",
      "24 episode finished after 11.0 rewards\n",
      "25 episode finished after 15.0 rewards\n",
      "26 episode finished after 11.0 rewards\n",
      "27 episode finished after 12.0 rewards\n",
      "28 episode finished after 27.0 rewards\n",
      "29 episode finished after 12.0 rewards\n",
      "30 episode finished after 10.0 rewards\n",
      "31 episode finished after 10.0 rewards\n",
      "32 episode finished after 10.0 rewards\n",
      "33 episode finished after 9.0 rewards\n",
      "34 episode finished after 9.0 rewards\n",
      "35 episode finished after 11.0 rewards\n",
      "36 episode finished after 12.0 rewards\n",
      "37 episode finished after 10.0 rewards\n",
      "38 episode finished after 8.0 rewards\n",
      "39 episode finished after 131.0 rewards\n",
      "40 episode finished after 41.0 rewards\n",
      "41 episode finished after 42.0 rewards\n",
      "42 episode finished after 51.0 rewards\n",
      "43 episode finished after 119.0 rewards\n",
      "44 episode finished after 132.0 rewards\n",
      "45 episode finished after 50.0 rewards\n",
      "46 episode finished after 103.0 rewards\n",
      "47 episode finished after 89.0 rewards\n",
      "48 episode finished after 55.0 rewards\n",
      "49 episode finished after 79.0 rewards\n",
      "50 episode finished after 55.0 rewards\n",
      "51 episode finished after 158.0 rewards\n",
      "52 episode finished after 145.0 rewards\n",
      "53 episode finished after 109.0 rewards\n",
      "54 episode finished after 67.0 rewards\n",
      "55 episode finished after 99.0 rewards\n",
      "56 episode finished after 269.0 rewards\n",
      "57 episode finished after 152.0 rewards\n",
      "58 episode finished after 84.0 rewards\n",
      "59 episode finished after 76.0 rewards\n",
      "60 episode finished after 124.0 rewards\n",
      "61 episode finished after 92.0 rewards\n",
      "62 episode finished after 110.0 rewards\n",
      "63 episode finished after 163.0 rewards\n",
      "64 episode finished after 160.0 rewards\n",
      "65 episode finished after 156.0 rewards\n",
      "66 episode finished after 123.0 rewards\n",
      "67 episode finished after 138.0 rewards\n",
      "68 episode finished after 122.0 rewards\n",
      "69 episode finished after 70.0 rewards\n",
      "70 episode finished after 393.0 rewards\n",
      "71 episode finished after 88.0 rewards\n",
      "72 episode finished after 111.0 rewards\n",
      "73 episode finished after 172.0 rewards\n",
      "74 episode finished after 104.0 rewards\n",
      "75 episode finished after 100.0 rewards\n",
      "76 episode finished after 232.0 rewards\n",
      "77 episode finished after 157.0 rewards\n",
      "78 episode finished after 91.0 rewards\n",
      "79 episode finished after 140.0 rewards\n",
      "80 episode finished after 60.0 rewards\n",
      "81 episode finished after 272.0 rewards\n",
      "82 episode finished after 72.0 rewards\n",
      "83 episode finished after 108.0 rewards\n",
      "84 episode finished after 60.0 rewards\n",
      "85 episode finished after 148.0 rewards\n",
      "86 episode finished after 139.0 rewards\n",
      "87 episode finished after 94.0 rewards\n",
      "88 episode finished after 51.0 rewards\n",
      "89 episode finished after 119.0 rewards\n",
      "90 episode finished after 188.0 rewards\n",
      "91 episode finished after 303.0 rewards\n",
      "92 episode finished after 341.0 rewards\n",
      "93 episode finished after 343.0 rewards\n",
      "94 episode finished after 72.0 rewards\n",
      "95 episode finished after 68.0 rewards\n",
      "96 episode finished after 49.0 rewards\n",
      "97 episode finished after 63.0 rewards\n",
      "98 episode finished after 68.0 rewards\n",
      "99 episode finished after 201.0 rewards\n"
     ]
    }
   ],
   "source": [
    "#Initialize replay memory D to capacity N\n",
    "memory = ExperienceReplay(replay_memory_size)\n",
    "\n",
    "#Initialize action-value function Q with random weights\n",
    "Q = NeuralNetwork().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(Q.parameters(), lr=learning_rate)\n",
    "\n",
    "reward_history = []\n",
    "total_steps = 0\n",
    "start_time = time.time()\n",
    "\n",
    "#for episode = 1, M do\n",
    "for i_episode in range(num_episodes):\n",
    "    #Initialize sequence s1={x1} and preprocessed sequenced f1=f(s1)\n",
    "    s = env.reset()\n",
    "    reward = 0\n",
    "    #for t=1, T do\n",
    "    while True:\n",
    "        total_steps += 1\n",
    "        \n",
    "        #With probability e select a random action a_t\n",
    "        #otherwise select a_t = max_a Q*(f(S_t),a;theta)\n",
    "        a = select_action(s, total_steps)\n",
    "\n",
    "        #Execute action a_t in emulator and observe reward r_t and image x_t+1\n",
    "        #Set s_t+1, a_t, x_t+1 and preprocesse f_t+1=f(s_t+1)\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        reward += r\n",
    "        env.render()\n",
    "        \n",
    "        #Store transition(f_t, a_t, r_t, f_t+1) in D\n",
    "        memory.push(s, a, s_, r, done)\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            #Sample random minibatch of transitions(f_j,a_j,r_j,f_j+1) from D\n",
    "            states, actions, new_states, rewards, dones = memory.sample(batch_size)\n",
    "\n",
    "            states = torch.Tensor(states).to(device)\n",
    "            actions = torch.LongTensor(actions).to(device)\n",
    "            new_states = torch.Tensor(new_states).to(device)\n",
    "            rewards = torch.Tensor([rewards]).to(device)\n",
    "            dones = torch.Tensor(dones).to(device)\n",
    "\n",
    "            new_action_values = Q(new_states).detach()\n",
    "\n",
    "            #set y_j=r_j for terminal f_j+1\n",
    "            # r_j + gamma*max_a'Q(f_j+1, a';theta) otherwise\n",
    "            # dones가 batch_size array 이므로 dones가 0 인경우의 acton_value 만 남김\n",
    "            y_target = rewards + \\\n",
    "                (1 - dones) * gamma * torch.max(new_action_values, 1)[0]\n",
    "\n",
    "            y_pred = Q(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "            #Perform a gradient descent reward on (y_j - Q(f_j+1, a_j;theta))^2\n",
    "            loss = criterion(y_pred.squeeze(), y_target.squeeze())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        s = s_\n",
    "\n",
    "        if done:\n",
    "            reward_history.append(reward)\n",
    "            print(f\"{i_episode} episode finished after {reward:.1f} rewards\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafce41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average steps: %.2f\" % (sum(reward_history)/num_episodes))\n",
    "print(\"Average of last 50 episodes: %.2f\" % (sum(reward_history[-50:])/50))\n",
    "print(\"---------------------- Hyper parameters --------------------------------------\")\n",
    "print(f\"gamma:{gamma}, learning rate: {learning_rate}, hidden layer: {hidden_layer}\")\n",
    "print(f\"replay_memory: {replay_memory_size}, batch size: {batch_size}\")\n",
    "print(f\"EPS_START: {EPS_START}, EPS_END: {EPS_END}, EPS_DECAY: {EPS_DECAY}\")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Time Elapsed : {elapsed_time//60} min {elapsed_time%60:.0} sec\")\n",
    "\n",
    "plt.bar(torch.arange(len(reward_history)).numpy(), reward_history)\n",
    "plt.xlabel(\"episodes\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.title(\"Deep Q-Learning\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
