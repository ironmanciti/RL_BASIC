{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTfA3OUb3Hcx"
      },
      "source": [
        "# 110. Deep Neural Network을 이용한 함수 근사에서 필요한 torch basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rXD5j_4_3Hc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30cc313-8249-44fa-dc7b-f5cd0520f953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import gym\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "env = gym.make('CartPole-v1')  \n",
        "action_size = env.action_space.n\n",
        "action_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uciS0nsN3Hc2"
      },
      "source": [
        "## Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hyL29xvt3Hc3"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  \n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", \n",
        "                                     field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(\n",
        "                        np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(\n",
        "                        np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(\n",
        "                        np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(\n",
        "                        np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(\n",
        "                        np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mBA3O2s3Hc4",
        "outputId": "e298887e-22c3-4f9c-a871-83c697938130"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "deque([Experience(state=array([-0.03634721, -0.0365727 , -0.0331441 ,  0.03930578]), action=0, reward=1.0, next_state=array([-0.03707866, -0.23120407, -0.03235799,  0.32134992]), done=False),\n",
              "       Experience(state=array([-0.03707866, -0.23120407, -0.03235799,  0.32134992]), action=0, reward=1.0, next_state=array([-0.04170274, -0.42585064, -0.02593099,  0.60365534]), done=False),\n",
              "       Experience(state=array([-0.04170274, -0.42585064, -0.02593099,  0.60365534]), action=1, reward=1.0, next_state=array([-0.05021975, -0.2303758 , -0.01385788,  0.3029188 ]), done=False),\n",
              "       Experience(state=array([-0.05021975, -0.2303758 , -0.01385788,  0.3029188 ]), action=0, reward=1.0, next_state=array([-0.05482727, -0.42529754, -0.00779951,  0.59119925]), done=False),\n",
              "       Experience(state=array([-0.05482727, -0.42529754, -0.00779951,  0.59119925]), action=0, reward=1.0, next_state=array([-0.06333322, -0.62030943,  0.00402448,  0.88141515]), done=False),\n",
              "       Experience(state=array([-0.06333322, -0.62030943,  0.00402448,  0.88141515]), action=1, reward=1.0, next_state=array([-0.07573941, -0.42524237,  0.02165278,  0.59000014]), done=False),\n",
              "       Experience(state=array([-0.07573941, -0.42524237,  0.02165278,  0.59000014]), action=0, reward=1.0, next_state=array([-0.08424426, -0.62066071,  0.03345279,  0.88942434]), done=False),\n",
              "       Experience(state=array([-0.08424426, -0.62066071,  0.03345279,  0.88942434]), action=1, reward=1.0, next_state=array([-0.09665747, -0.42600828,  0.05124127,  0.60744234]), done=False),\n",
              "       Experience(state=array([-0.09665747, -0.42600828,  0.05124127,  0.60744234]), action=0, reward=1.0, next_state=array([-0.10517764, -0.6218078 ,  0.06339012,  0.91581447]), done=False),\n",
              "       Experience(state=array([-0.10517764, -0.6218078 ,  0.06339012,  0.91581447]), action=1, reward=1.0, next_state=array([-0.11761379, -0.42759771,  0.08170641,  0.64370866]), done=False)])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#Initialize replay memory D to capacity N\n",
        "BUFFER_SIZE = 500\n",
        "BATCH_SIZE = 3\n",
        "seed=0\n",
        "\n",
        "replay_memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "s = env.reset()\n",
        "for i in range(10):\n",
        "    a = env.action_space.sample()\n",
        "    s_, r, done, _ = env.step(a)\n",
        "    replay_memory.add(s, a, r, s_, done)\n",
        "    s = s_\n",
        "    \n",
        "replay_memory.memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBWTiu01BTOB"
      },
      "source": [
        "## Sample random minibatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t4tHzM6BO4_",
        "outputId": "74d7419c-16bd-4254-8ded-71c8b88b8f62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.0757, -0.4252,  0.0217,  0.5900],\n",
              "         [-0.1052, -0.6218,  0.0634,  0.9158],\n",
              "         [-0.0363, -0.0366, -0.0331,  0.0393]], device='cuda:0'), tensor([[0],\n",
              "         [1],\n",
              "         [0]], device='cuda:0'), tensor([[1.],\n",
              "         [1.],\n",
              "         [1.]], device='cuda:0'), tensor([[-0.0842, -0.6207,  0.0335,  0.8894],\n",
              "         [-0.1176, -0.4276,  0.0817,  0.6437],\n",
              "         [-0.0371, -0.2312, -0.0324,  0.3213]], device='cuda:0'), tensor([[0.],\n",
              "         [0.],\n",
              "         [0.]], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "states, actions, rewards, next_states, dones = replay_memory.sample()\n",
        "\n",
        "states, actions, rewards, next_states, dones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-xKKrqw3Hc5"
      },
      "source": [
        "## Select Action \n",
        "\n",
        "- state가 4 개의 feature로 구성되고 각 state에서의 action이 2 가지인 MDP의 parameter화 된 state action value function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gbVFe-jz3Hc6"
      },
      "outputs": [],
      "source": [
        "n_inputs = 4  # state feature\n",
        "n_outputs = 2  # action space\n",
        "hidden_layer = 64\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear1 = nn.Linear(n_inputs, hidden_layer)\n",
        "        self.linear2 = nn.Linear(hidden_layer, n_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a1 = torch.relu(self.linear1(x))\n",
        "        output = self.linear2(a1)\n",
        "        return output\n",
        "\n",
        "Q = NeuralNetwork().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0fir5Pl3Hc7"
      },
      "source": [
        "- 입력 : 4 개 feature 로 구성된 state \n",
        "- 출력 : 2 개 action values  \n",
        "\n",
        "- greedy action : $max_{a'}Q(s', a';\\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ1VsEkd3Hc7",
        "outputId": "c08766e7-27a2-4d04-f399-c3fd1d5e0584"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0737, -0.0007], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "input = [0.1, 0.2, 0.3, 0.4]\n",
        "\n",
        "action_values = Q(torch.tensor(input).to(device))\n",
        "action_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTXrKPAk3Hc8",
        "outputId": "ac2fd21b-945a-4ce1-f18c-31ecb8971e3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# greedy action\n",
        "action = torch.argmax(action_values).item() \n",
        "action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dILVsp1p3Hc9"
      },
      "source": [
        "## State-Action Value (q value) from DQN \n",
        "\n",
        "Q-network 에서 입력으로 주어진 states 에 대응하는 action values 를 출력으로 얻어 greedy action 을 선택하는 code.  \n",
        "\n",
        "함수 max()는 최대값과 해당 값의 인덱스를 모두 반환하므로 최대값과 argmax를 모두 계산합니다. 이 경우 값에만 관심이 있기 때문에 결과의 첫 번째 항목(values)을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NfWJLj63Hc-",
        "outputId": "8b454cf3-5f34-4100-ce6e-a503e8173de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1251, -0.0336],\n",
            "        [ 0.2277, -0.0451],\n",
            "        [-0.0629, -0.0509]])\n",
            "torch.return_types.max(\n",
            "values=tensor([ 0.1251,  0.2277, -0.0509]),\n",
            "indices=tensor([0, 0, 1]))\n",
            "\n",
            "tensor([ 0.1251,  0.2277, -0.0509])\n",
            "tensor([0, 0, 1])\n"
          ]
        }
      ],
      "source": [
        "action_values = Q(states).cpu().detach()\n",
        "\n",
        "print(action_values)\n",
        "print(torch.max(action_values, dim=1))\n",
        "print()\n",
        "\n",
        "values, indices = torch.max(action_values, dim=1)\n",
        "\n",
        "print(values)\n",
        "print(indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSugiYLx3Hc-"
      },
      "source": [
        "## torch.gather\n",
        "\n",
        "- torch.gather 함수 (또는 torch.Tensor.gather)는 다중 인덱스 선택 방법  \n",
        "\n",
        "- 첫 번째 인수 인 input은 요소를 선택하려는 소스 텐서. 두 번째 dim은 수집하려는 차원. 마지막으로 index는 입력을 인덱싱하는 인덱스.\n",
        "\n",
        "4개의 항목과 4개의 작업으로 구성된 일괄 처리가 있는 간단한 예제 사례에서 gather가 수행하는 작업의 요약입니다.\n",
        "\n",
        "```\n",
        "state_action_values = net(states_v).gather(1, actions_v.unsqueeze(1))\n",
        "```\n",
        "\n",
        "\n",
        "<img src=https://miro.medium.com/max/1400/1*fS-9p5EBKVgl69Gy0gwjGQ.png width=400>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsSGQnQa3Hc_",
        "outputId": "9c50f027-6290-4904-8017-ee63234e88aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0757, -0.4252,  0.0217,  0.5900],\n",
              "        [-0.1052, -0.6218,  0.0634,  0.9158],\n",
              "        [-0.0363, -0.0366, -0.0331,  0.0393]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "states  # 4개의 feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNSRn_CT3Hc_",
        "outputId": "583135fa-96bd-4e43-e333-8b3a314e1363"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1251, -0.0336],\n",
              "        [ 0.2277, -0.0451],\n",
              "        [-0.0629, -0.0509]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "q_values = Q(states)\n",
        "q_values  # 2 개의 action values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsIuw19X3Hc_",
        "outputId": "aaf625d1-bc81-4d94-be0b-4174fb1892db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1],\n",
              "        [0],\n",
              "        [1]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "action = torch.LongTensor([1, 0, 1]).unsqueeze(1).to(device)\n",
        "action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXOP_jIq3HdA",
        "outputId": "a58decc9-1adb-4ad1-b6be-e52478af0506"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0336],\n",
              "        [ 0.2277],\n",
              "        [-0.0509]], device='cuda:0', grad_fn=<GatherBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "torch.gather(q_values, 1, action)  #q_value의 axis=1에서 action index 수집"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNvF6oCP3HdA",
        "outputId": "7be184f1-fd48-4dba-d1e3-fa2421c7a1aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0336],\n",
              "        [ 0.2277],\n",
              "        [-0.0509]], device='cuda:0', grad_fn=<GatherBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "q_values.gather(1, action)   # 위와 동일 operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufB1JYnC3HdB"
      },
      "source": [
        "## REINFORECE 알고리즘 지원을 위한 PROBABILITY DISTRIBUTIONS - TORCH.DISTRIBUTIONS\n",
        "\n",
        "- distribution 패키지에는 매개변수화할 수 있는 확률 분포와 sampling 함수가 포함되어 있습니다. 이를 통해 최적화를 위한 확률적 계산 그래프 및 확률적 기울기 추정기를 구성할 수 있습니다. \n",
        "\n",
        "- torch 는 다음과 같이 REINFORCE 알고리즘을 지원합니다.\n",
        "\n",
        "```python\n",
        "    probs = policy_network(state)\n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "    next_state, reward = env.step(action)\n",
        "    loss = -m.log_prob(action) * reward\n",
        "    loss.backward()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEN8t3cq3HdB"
      },
      "source": [
        "### 방법 1) Categorical(probs) 에서 sampling\n",
        "\n",
        "'probs'가 길이가 'K'인 1차원 array인 경우, 각 element 는 해당 인덱스에서 클래스를 샘플링할 상대 확률입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRFyebJE3HdB",
        "outputId": "98223f29-3e0b-4dc3-ecc5-682cb2f477dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax 확률 분포 : tensor([0.1533, 0.3354, 0.2068, 0.3045]), sum = 0.9999998807907104\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Categorical(probs: torch.Size([4]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import torch\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "logits = torch.rand(4)\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "print(f\"softmax 확률 분포 : {probs}, sum = {probs.sum()}\")\n",
        "\n",
        "# 각 class 를 sampling 할 상대 확률\n",
        "m = Categorical(probs)\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TACqKr0Y3HdC"
      },
      "source": [
        "위의 m 에서 sampling 을 반복하면 softmax 확률 분포로 sampling 된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHvhhzZO3HdC",
        "outputId": "3e2e55c8-868a-4315-e11d-c5cfce92dc4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 4607), (1, 9959), (2, 6245), (3, 9189)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from collections import Counter\n",
        "samples = []\n",
        "\n",
        "for _ in range(30000):\n",
        "    a = m.sample()\n",
        "    samples.append(a.item())\n",
        "\n",
        "sorted(Counter(samples).items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muu7DisY3HdC"
      },
      "source": [
        "### 방법 2) np.random.choice 에서 sampling\n",
        "\n",
        "- np.random.choice 의 `parameter p`에 softmax 확률 분포 지정하여 sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewhGZw5w3HdC",
        "outputId": "bc228a97-4870-4343-cae7-645ae0bff82e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 4601), (1, 10057), (2, 6273), (3, 9069)]\n"
          ]
        }
      ],
      "source": [
        "samples = []\n",
        "\n",
        "for _ in range(30000):\n",
        "    a = np.random.choice(4, p=probs.numpy())\n",
        "    samples.append(a)\n",
        "    \n",
        "print(sorted(Counter(samples).items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13-ePWwd3HdD"
      },
      "source": [
        "### REINFORCE 구현을  위해  total expected return $G_t$ 를 estimate 하는 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dA4SgEC23HdD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "rewards = [1, 2, 3, 4, 5]\n",
        "gamma = 0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgyPm6P-3HdD",
        "outputId": "5a2c6d26-e9c2-46a1-9d8e-70ce7caad763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.604476049999999 13.741895 11.8605 8.95 5\n"
          ]
        }
      ],
      "source": [
        "G_0 = 1 + 0.99 * 2 + 0.99**2*3 + 0.99**3*4 + 0.99**4*5\n",
        "G_1 = 2 + 0.99**1*3 + 0.99**2*4 + 0.99**3*5\n",
        "G_2 = 3 + 0.99**1*4 + 0.99**2*5\n",
        "G_3 = 4 + 0.99**1*5\n",
        "G_4 = 5\n",
        "print(G_0, G_1, G_2, G_3, G_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fcFvCy3I3HdD"
      },
      "outputs": [],
      "source": [
        "r = np.array([gamma**i * rewards[i]\n",
        "              for i in range(len(rewards))])\n",
        "# Reverse the array direction for cumsum and then\n",
        "# revert back to the original order\n",
        "r = r[::-1].cumsum()[::-1]\n",
        "# return r - r.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkFx4vW13HdD",
        "outputId": "e5fff025-cbe8-4d41-a3a7-e25e12393d9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14.60447605, 13.741895  , 11.8605    ,  8.95      ,  5.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# episodic task\n",
        "Returns = []\n",
        "G = 0\n",
        "for r in rewards[::-1]:\n",
        "    G = r + gamma * G\n",
        "    Returns.append(G)\n",
        "    \n",
        "Returns = np.array(Returns[::-1], dtype=np.float64)\n",
        "Returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlrV5LBC3HdE",
        "outputId": "f4289481-6034-4738-f441-fdb900a3a6e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.60447605 13.741895   11.8605      8.95        5.        ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.77310184,  2.91052079,  1.02912579, -1.88137421, -5.83137421])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# continuing task\n",
        "def discount_rewards(rewards):\n",
        "    Returns = []\n",
        "    G = 0\n",
        "    for r in rewards[::-1]:\n",
        "        G = r + gamma * G\n",
        "        Returns.append(G)\n",
        "    # cumsum의 배열 방향을 반대로 한 다음 원래 순서로 되돌립니다.\n",
        "    Returns = np.array(Returns[::-1], dtype=np.float64)\n",
        "    print(Returns)\n",
        "    return Returns - Returns.mean()\n",
        "\n",
        "discount_rewards(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simlDDmD3HdE"
      },
      "source": [
        "### REINFORCE 구현을 위한 Score Function\n",
        "\n",
        "- 확률 밀도 함수가 매개 변수와 관련하여 미분할 수있는 경우 REINFORCE를 구현하려면 sample () 및 log_prob () 만 필요\n",
        "\n",
        "$$\\Delta_{\\theta} = \\alpha r \\frac{\\partial log p(a | \\pi^{\\theta}(s))}{\\partial\\theta}$$  \n",
        "\n",
        "$\\alpha$ - learning rate, r - reward,  $p(a|\\pi^\\theta(s))$ - probability of taking action a  \n",
        "\n",
        "\n",
        "- Network 출력에서 action을 샘플링하고 이 action을 environment에 적용한 다음 log_prob를 사용하여 동등한 손실 함수를 구성.   \n",
        "- optimizer는 경사 하강법을 사용하기 때문에 음수를 사용하는 반면 위의 규칙은 경사 상승을 가정.   \n",
        "- Categorical Policy를 사용하는 경우 REINFORCE를 구현하는 코드는 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhVDiCjk3HdE",
        "outputId": "77da07fc-a3f6-4897-9e19-3f837cf924a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8201)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1')  \n",
        "s = env.reset()\n",
        "\n",
        "#probs = policy_network(state)\n",
        "logits = torch.rand(2)\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "m = Categorical(probs)\n",
        "action = m.sample()\n",
        "\n",
        "next_state, reward, done, _ = env.step(action.item())\n",
        "\n",
        "loss = -m.log_prob(action) * reward\n",
        "#loss.backward()\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZhEg-H53HdE"
      },
      "source": [
        "## Huber Loss\n",
        "\n",
        "- Actor-Critic 의 critic value function 의 loss 계산에 사용  \n",
        "- Huber Loss는 L1과 L2의 장점을 취하면서 단점을 보완하기 위해서 제안된 것이 Huber Loss다.\n",
        "    - 모든 지점에서 미분이 가능하다.  \n",
        "    - Outlier에 상대적으로 Robust하다.\n",
        "<img src=https://bekaykang.github.io/assets/img/post/201209-2.png width=300>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn8JMIvJ3HdF",
        "outputId": "64fbcd18-6ad3-4227-e2ba-3fd8d6eab8e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.9000)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "curr_q = torch.FloatTensor([10,11,12,10,9])\n",
        "target_q = torch.FloatTensor([12,8,10,13,11])\n",
        "loss = F.smooth_l1_loss(curr_q, target_q)\n",
        "print(loss)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "110_basic_operations_for_Function_Approximation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}