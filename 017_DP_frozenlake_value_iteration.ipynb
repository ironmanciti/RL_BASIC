{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efbf2894",
   "metadata": {},
   "source": [
    "# 017_DP_frozenlake_value_iteration\n",
    "\n",
    "# Value Iteration\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "nA = 4\n",
    "nS = 4*4 = 16\n",
    "P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "env.P[0][0] \n",
    "{0: {0: [(0.3333333333333333, 0, 0.0, False), --> (P[s'], s', r, done)\n",
    "         (0.3333333333333333, 0, 0.0, False),\n",
    "         (0.3333333333333333, 4, 0.0, False)],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37df27bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 4, 0.0, False)],\n",
       "  2: [(1.0, 1, 0.0, False)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 1: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 5, 0.0, True)],\n",
       "  2: [(1.0, 2, 0.0, False)],\n",
       "  3: [(1.0, 1, 0.0, False)]},\n",
       " 2: {0: [(1.0, 1, 0.0, False)],\n",
       "  1: [(1.0, 6, 0.0, False)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 3: {0: [(1.0, 2, 0.0, False)],\n",
       "  1: [(1.0, 7, 0.0, True)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 3, 0.0, False)]},\n",
       " 4: {0: [(1.0, 4, 0.0, False)],\n",
       "  1: [(1.0, 8, 0.0, False)],\n",
       "  2: [(1.0, 5, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(1.0, 5, 0.0, True)],\n",
       "  1: [(1.0, 10, 0.0, False)],\n",
       "  2: [(1.0, 7, 0.0, True)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 12, 0.0, True)],\n",
       "  2: [(1.0, 9, 0.0, False)],\n",
       "  3: [(1.0, 4, 0.0, False)]},\n",
       " 9: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 10, 0.0, False)],\n",
       "  3: [(1.0, 5, 0.0, True)]},\n",
       " 10: {0: [(1.0, 9, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 11, 0.0, True)],\n",
       "  3: [(1.0, 6, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(1.0, 12, 0.0, True)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 14, 0.0, False)],\n",
       "  3: [(1.0, 9, 0.0, False)]},\n",
       " 14: {0: [(1.0, 13, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 15, 1.0, True)],\n",
       "  3: [(1.0, 10, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f913d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = len(env.P)\n",
    "num_actions = len(env.P[0])\n",
    "transitions = env.P "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110e5b5",
   "metadata": {},
   "source": [
    "<img src=\"https://jaydottechdotblog.files.wordpress.com/2016/12/rl-value-iteration-algorithm.png\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e6499cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 1.0\n",
    "THETA = 1e-5\n",
    "\n",
    "# 1. initialize array V(s) = 0 for all s in S+\n",
    "V = np.zeros(num_states)\n",
    "\n",
    "#Loop\n",
    "while True:\n",
    "    #delta <- 0\n",
    "    delta = 0\n",
    "    #Loop for each s\n",
    "    for s in range(num_states):\n",
    "        old_value = V[s]\n",
    "        new_action_values = np.zeros(num_actions)\n",
    "        \n",
    "        #V(s) = max_a(sum(p(s,a)*[r + gamma*v(s')]))\n",
    "        for a in range(num_actions):\n",
    "            # sum over s', r\n",
    "            for prob, s_, r, _ in transitions[s][a]:\n",
    "                new_action_values[a] += prob * (r + GAMMA * V[s_]) / num_actions\n",
    "        V[s] = max(new_action_values)\n",
    "        \n",
    "        #delta <-max(delta|v - V(s)|)\n",
    "        delta = max(delta, np.abs(old_value - V[s]))\n",
    "    #until delta < theta\n",
    "    if delta < THETA:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "645f8096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.44140625e-04, 9.76562500e-04, 3.90625000e-03, 9.76562500e-04,\n",
       "       9.76562500e-04, 0.00000000e+00, 1.56250000e-02, 0.00000000e+00,\n",
       "       3.90625000e-03, 1.56250000e-02, 6.25000000e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 6.25000000e-02, 2.50000000e-01, 0.00000000e+00])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac76d02",
   "metadata": {},
   "source": [
    "action 값을 이용하여 결정론적 최적 정책 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af9aa38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract deterministic optimal policy using action value\n",
    "pi = np.zeros((num_states, num_actions))\n",
    "\n",
    "for s in range(num_states):\n",
    "    action_values = np.zeros(num_actions)\n",
    "    \n",
    "    for a in range(num_actions):\n",
    "        # sum over s', r\n",
    "        for prob, s_, r, _ in transitions[s][a]:\n",
    "            action_values[a] += prob * (r + GAMMA * V[s_])\n",
    "            #pi(s) <- argmax_a(action_values)\n",
    "            new_action = np.argmax(action_values)\n",
    "            pi[s] = np.eye(num_actions)[new_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b741168f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy = \n",
      " [[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "SFFF       (S: starting point, safe)\n",
      "FHFH       (F: frozen surface, safe)\n",
      "FFFH       (H: hole, fall to your doom)\n",
      "HFFG       (G: goal, where the frisbee is located)\n",
      "\n",
      "LEFT = 0\n",
      "DOWN = 1\n",
      "RIGHT = 2\n",
      "UP = 3\n",
      "    \n",
      "Optimal Action = \n",
      " [[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Policy = \\n\", pi)\n",
    "print(\n",
    "    \"\"\"\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "    \"\"\")\n",
    "print(\"Optimal Action = \\n\", np.argmax(pi, axis=1).reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df0cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
