{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a5c90a",
   "metadata": {},
   "source": [
    "# 016_DP_frozenlake_policy_iteration\n",
    "\n",
    "# Policy Iteration\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "nA = 4\n",
    "nS = 4*4 = 16\n",
    "P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "env.P[0][0] \n",
    "{0: {0: [(0.3333333333333333, 0, 0.0, False), --> (P[s'], s', r, done)\n",
    "         (0.3333333333333333, 0, 0.0, False),\n",
    "         (0.3333333333333333, 4, 0.0, False)],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b71bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 4, 0.0, False)],\n",
       "  2: [(1.0, 1, 0.0, False)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 1: {0: [(1.0, 0, 0.0, False)],\n",
       "  1: [(1.0, 5, 0.0, True)],\n",
       "  2: [(1.0, 2, 0.0, False)],\n",
       "  3: [(1.0, 1, 0.0, False)]},\n",
       " 2: {0: [(1.0, 1, 0.0, False)],\n",
       "  1: [(1.0, 6, 0.0, False)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 3: {0: [(1.0, 2, 0.0, False)],\n",
       "  1: [(1.0, 7, 0.0, True)],\n",
       "  2: [(1.0, 3, 0.0, False)],\n",
       "  3: [(1.0, 3, 0.0, False)]},\n",
       " 4: {0: [(1.0, 4, 0.0, False)],\n",
       "  1: [(1.0, 8, 0.0, False)],\n",
       "  2: [(1.0, 5, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(1.0, 5, 0.0, True)],\n",
       "  1: [(1.0, 10, 0.0, False)],\n",
       "  2: [(1.0, 7, 0.0, True)],\n",
       "  3: [(1.0, 2, 0.0, False)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 12, 0.0, True)],\n",
       "  2: [(1.0, 9, 0.0, False)],\n",
       "  3: [(1.0, 4, 0.0, False)]},\n",
       " 9: {0: [(1.0, 8, 0.0, False)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 10, 0.0, False)],\n",
       "  3: [(1.0, 5, 0.0, True)]},\n",
       " 10: {0: [(1.0, 9, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 11, 0.0, True)],\n",
       "  3: [(1.0, 6, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(1.0, 12, 0.0, True)],\n",
       "  1: [(1.0, 13, 0.0, False)],\n",
       "  2: [(1.0, 14, 0.0, False)],\n",
       "  3: [(1.0, 9, 0.0, False)]},\n",
       " 14: {0: [(1.0, 13, 0.0, False)],\n",
       "  1: [(1.0, 14, 0.0, False)],\n",
       "  2: [(1.0, 15, 1.0, True)],\n",
       "  3: [(1.0, 10, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e968ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = len(env.P)\n",
    "num_actions = len(env.P[0])\n",
    "transitions = env.P "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ccd7c",
   "metadata": {},
   "source": [
    "<img src=\"https://jaydottechdotblog.files.wordpress.com/2016/12/rl-policy-iteration-algorithm.png?w=730\" width=600/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a37e154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 1.0\n",
    "THETA = 1e-5\n",
    "\n",
    "# 1. initialize an array V(s) = 0 for all s in S+\n",
    "# and arbitrary pi(s) for all a in A+ for all s in S+\n",
    "V = np.zeros(num_states)\n",
    "pi = np.ones([num_states, num_actions]) * 0.25\n",
    "\n",
    "policy_stable = False\n",
    " \n",
    "while not policy_stable:\n",
    "    #2. Policy Evaluation\n",
    "    while True:  \n",
    "        #delta <- 0\n",
    "        delta = 0\n",
    "        #Loop for each s\n",
    "        for s in range(num_states):\n",
    "            old_value = V[s]\n",
    "            new_value = 0\n",
    "            #V(s) = sum(pi(a|s)*sum(p(s,a)*[r + gamma*v(s')]))\n",
    "            for a, prob_a in enumerate(pi[s]):\n",
    "                # sum over s', r\n",
    "                for prob, s_, r, _ in transitions[s][a]:\n",
    "                    new_value += prob_a * prob * (r + GAMMA * V[s_])\n",
    "            V[s] = new_value\n",
    "            #delta <-max(delta|v - V(s)|)\n",
    "            delta = max(delta, np.abs(old_value - V[s]))\n",
    "        #until delta < theta\n",
    "        if delta < THETA:\n",
    "            break\n",
    "        \n",
    "    #3. Policy Improvement\n",
    "    #policy_stable <- true   \n",
    "    policy_stable = True\n",
    "    old_policy = pi\n",
    "    #For each s:\n",
    "    for s in range(num_states):\n",
    "        # pi_s <- argmax_a(sum(p(s',r|s,a)*[r + gamma*V(s')]))\n",
    "        new_action_values = np.zeros(num_actions)\n",
    "        for a in range(num_actions):\n",
    "            for prob, s_, r, _ in transitions[s][a]:\n",
    "                new_action_values[a] += prob * (r + GAMMA * V[s_])\n",
    "                        \n",
    "        new_action = np.argmax(new_action_values)\n",
    "        pi[s] = np.eye(num_actions)[new_action]\n",
    "\n",
    "    if old_policy.all() != pi.all():\n",
    "        policy_stable = False\n",
    "    #If policy-stable. then stop and return V and pi; else go to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0298092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value = \n",
      " [[0.013911   0.01161424 0.02094062 0.01046758]\n",
      " [0.01623478 0.         0.04074774 0.        ]\n",
      " [0.03479961 0.08816698 0.14205099 0.        ]\n",
      " [0.         0.17581855 0.4392897  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "V = V.reshape(4, 4)\n",
    "print(\"Optimal Value = \\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf8d56e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy = \n",
      " [[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "SFFF       (S: starting point, safe)\n",
      "FHFH       (F: frozen surface, safe)\n",
      "FFFH       (H: hole, fall to your doom)\n",
      "HFFG       (G: goal, where the frisbee is located)\n",
      "\n",
      "LEFT = 0\n",
      "DOWN = 1\n",
      "RIGHT = 2\n",
      "UP = 3\n",
      "    \n",
      "Optimal Action = \n",
      " [[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Policy = \\n\", pi)\n",
    "print(\n",
    "    \"\"\"\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "    \"\"\")\n",
    "print(\"Optimal Action = \\n\", np.argmax(pi, axis=1).reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cdee24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
